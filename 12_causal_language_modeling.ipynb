{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694afc15",
   "metadata": {},
   "source": [
    "There are two types of language modeling, causal and masked. Causal language models are frequently used for text generation. These models can be used for creative applications like choosing your own text adventure or for an intelligent coding assistant like Copilot or CodeParrot.\n",
    "\n",
    "Causal language modeling predicts the next token in a sequence of tokens, and the model can only attend to tokens on the left. This means the model cannot see future tokens. GPT-2 is an example of a causal language model.\n",
    "\n",
    "This guide illustrates how to:\n",
    "1. Finetune DistilGPT2 on the r/askscience subset of the ELI5 dataset.\n",
    "2. Use the finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c231f",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3444ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: evaluate in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e692c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01304de",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2b8094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '7414mj',\n",
       " 'title': 'Voices usually sound tired early in the morning. What\\'s the best way to \"wake up\" our voices quickly?',\n",
       " 'selftext': '',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['dnuxftq', 'dnuntk5'],\n",
       "  'text': ['Vocal chords, much like muscles, need to be warmed up to be used at max efficiency. As a singer, every morning I do vocal warm-up exercises to make sure I\\'m not destroying my voice when I sing. I also work at a call center, so preparing my voice for the day makes it much more manageable to speak on the phone for hours on a daily basis. However, to more directly answer your question, there isn\\'t a \"trick\" that will wake up your voice immediately. Doing some quick exercises can help your voice warm up more quickly, though. There are a lot of YouTube videos on these, but my favorite are \"bubbles\". I would provide links but I\\'m at work and on mobile.',\n",
       "   'So I\\'m not sure there is a scientific way, but I\\'d say practice makes perfect, i tended to be late for things from time to time and get calls like \"where are you, are you still in bed?\" Kind of stuff. So before you answer the phone, Start off with a low growl and then raise it till you have kind of a shout. This should clear any flem and give your vocals a nice stretch. Secondly practice a \"retail\" voice, sharpness and clarity is key. And lazyness that\\'s the driving factor behind it all!!'],\n",
       "  'score': [17, 8],\n",
       "  'text_urls': [[], []]},\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a smaller subset of the r/askscience subset of the ELI5 dataset from the ðŸ¤— Datasets library\n",
    "#Â Experiment and make sure everything works before spending more time training on the full dataset\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "\n",
    "# Inspect an example\n",
    "# NB: the output may look like a lot, but weâ€™re only really interested in the text field\n",
    "# This is an unsupervised task. Labels not required because the next word is the label.\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b23e0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff9ade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57799774fea345878d885a21fa3ef3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e2f22e44b84af394024b359dfd717a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e38407172ca43a2bea7c6d2f69119c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56368b369030486582ca52ca674e63ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load DistilGPT2 tokenizer to process the 'text' subfield\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be62328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '7414mj',\n",
       " 'title': 'Voices usually sound tired early in the morning. What\\'s the best way to \"wake up\" our voices quickly?',\n",
       " 'selftext': '',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers.a_id': ['dnuxftq', 'dnuntk5'],\n",
       " 'answers.text': ['Vocal chords, much like muscles, need to be warmed up to be used at max efficiency. As a singer, every morning I do vocal warm-up exercises to make sure I\\'m not destroying my voice when I sing. I also work at a call center, so preparing my voice for the day makes it much more manageable to speak on the phone for hours on a daily basis. However, to more directly answer your question, there isn\\'t a \"trick\" that will wake up your voice immediately. Doing some quick exercises can help your voice warm up more quickly, though. There are a lot of YouTube videos on these, but my favorite are \"bubbles\". I would provide links but I\\'m at work and on mobile.',\n",
       "  'So I\\'m not sure there is a scientific way, but I\\'d say practice makes perfect, i tended to be late for things from time to time and get calls like \"where are you, are you still in bed?\" Kind of stuff. So before you answer the phone, Start off with a low growl and then raise it till you have kind of a shout. This should clear any flem and give your vocals a nice stretch. Secondly practice a \"retail\" voice, sharpness and clarity is key. And lazyness that\\'s the driving factor behind it all!!'],\n",
       " 'answers.score': [17, 8],\n",
       " 'answers.text_urls': [[], []],\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice the 'text' subfield is actually nested inside answers. \n",
    "# extract the 'text' subfield from its nested structure with the flatten method\n",
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798c686a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe3043634c949b683675d09a15557d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1793 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57b2a0744ed487cbb76d0203d880c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1592 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1282 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1330 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2066 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# after flattening, text is now its own field - answers.text\n",
    "# Instead of tokenizing each sentence separately, convert the list to a string so you can jointly tokenize them\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7f269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
