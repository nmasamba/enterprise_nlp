{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694afc15",
   "metadata": {},
   "source": [
    "There are two types of language modeling, causal and masked. Causal language models are frequently used for text generation. These models can be used for creative applications like choosing your own text adventure or for an intelligent coding assistant like Copilot or CodeParrot.\n",
    "\n",
    "Causal language modeling predicts the next token in a sequence of tokens, and the model can only attend to tokens on the left. This means the model cannot see future tokens. GPT-2 is an example of a causal language model.\n",
    "\n",
    "This guide illustrates how to:\n",
    "1. Finetune DistilGPT2 on the r/askscience subset of the ELI5 dataset.\n",
    "2. Use the finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c231f",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e692c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\n",
    "AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01304de",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset of the r/askscience subset of the ELI5 dataset from the ðŸ¤— Datasets library\n",
    "#Â Experiment and make sure everything works before spending more time training on the full dataset\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "\n",
    "# Inspect an example\n",
    "# NB: the output may look like a lot, but weâ€™re only really interested in the text field\n",
    "# This is an unsupervised task. Labels not required because the next word is the label.\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8991dd",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilGPT2 tokenizer to process the 'text' subfield\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the 'text' subfield is actually nested inside answers. \n",
    "# extract the 'text' subfield from its nested structure with the flatten method\n",
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after flattening, text is now its own field - answers.text\n",
    "# Instead of tokenizing each sentence separately, convert the list to a string so you can jointly tokenize them\n",
    "# Note that unrequired columns can be removed during this step\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=24,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset now contains the token sequences...\n",
    "# but some of these are longer than the maximum input length for the model\n",
    "# Define block_size for splitting; should be shorter than the maximum input length but short enough for GPU RAM\n",
    "block_size = 128\n",
    "\n",
    "# Second preprocessing function to concatenate all the sequences\n",
    "# split the concatenated sequences into shorter chunks defined by block_size\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb067a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of examples with data collator\n",
    "# NB: Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation...\n",
    "# instead of padding the whole dataset to the maximum length\n",
    "\n",
    "# Use the end-of-sequence token as the padding token and set mlm=False. \n",
    "# This will use the inputs as labels shifted to the right by one element\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955dcd1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cdce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"causal_language_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c677e1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48e69e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109eaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model for inference\n",
    "#Â Create a prompt to generate text from\n",
    "prompt = \"Somatic hypermutation allows the immune system to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using a pipeline object\n",
    "generator = pipeline(\"text-generation\", model=\"causal_language_model\")\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68e358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
