{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f9b30a",
   "metadata": {},
   "source": [
    "There are two types of language modeling - causal and masked. \n",
    "\n",
    "Causal language models are frequently used for text generation. These models can be used for creative applications like choosing your own text adventure or for an intelligent coding assistant like Copilot or CodeParrot. Masked language models predict a masked token in a sequence, and the model can attend to tokens bidirectionally. This means the model has full access to the tokens on the left and right. Masked language modeling is great for tasks that require a good contextual understanding of an entire sequence. BERT is an example of a masked language model. \n",
    "\n",
    "This guide illustrates how to:\n",
    "1. Finetune DistilRoBERTa on the r/askscience subset of the ELI5 dataset.\n",
    "2. Use finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbf450",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a910e6",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba639e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset of the r/askscience subset of the ELI5 dataset from the ðŸ¤— Datasets library\n",
    "#Â Experiment and make sure everything works before spending more time training on the full dataset\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "\n",
    "# Inspect an example\n",
    "# NB: the output may look like a lot, but weâ€™re only really interested in the text field\n",
    "# This is an unsupervised task. Labels not required because the next word is the label.\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46eca5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a DistilRoBERTa tokenizer to process the text subfield\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text field is actually nested inside answers\n",
    "# Extract the text subfield from its nested structure with the flatten method\n",
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b182e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â text field is now a list\n",
    "# convert the list to a string to jointly tokenize them\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=24,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these are longer than the maximum input length for the model, which we'll correct below\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Drop the small remainder; we could add padding if the model supported it instead of this drop\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result \n",
    "\n",
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa122e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of examples using DataCollatorForLanguageModeling\n",
    "# Dynamically pad the sentences to the longest length in a batch during collation\n",
    "# Use the end-of-sequence token as the padding token\n",
    "#Â specify mlm_probability to randomly mask tokens during each iteration over the data\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6599a8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fe832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters in TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"masked_language_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Pass the training arguments to Trainer along with the model, datasets, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Call train() to finetune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed0def",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f113d6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come up with some text youâ€™d like the model to fill in the blank with\n",
    "# use the special <mask> token to indicate the blank\n",
    "text = \"The sun is our <mask> star.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26809db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Try finetuned model for inference in a pipeline()\n",
    "#Â use fill-mask with model, and pass your text to it\n",
    "# use the top_k parameter to specify how many predictions to return\n",
    "mask_filler = pipeline(\"fill-mask\", \"masked_language_model\")\n",
    "mask_filler(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Inference with PyTorch objects  \n",
    "# Tokenize the text and return the input_ids as PyTorch tensors\n",
    "# Specify the position of the <mask> token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"masked_language_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Pass inputs to the model and return the logits of the masked token\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"masked_language_model\")\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "# Return the three masked tokens with the highest probability and print them out\n",
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
