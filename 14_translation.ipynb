{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da3e0d0",
   "metadata": {},
   "source": [
    "Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem - a powerful framework for returning some output from an input, like translation or summarization. Translation systems are commonly used for translation between different language texts, but it can also be used for speech or some combination in between like text-to-speech or speech-to-text.\n",
    "\n",
    "This guide shows how to:\n",
    "1. Finetune T5 on the English-French subset of the OPUS Books dataset to translate English text to French.\n",
    "2. Use the finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e55bc",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b259167",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac0b19",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f4d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)\n",
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9fb0f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a T5 tokenizer to process the English-French language pairs\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prefix the input with a prompt so T5 knows this is a translation task\n",
    "# Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the input (English) and target (French) separately \n",
    "    # We can’t tokenize French text with a tokenizer pretrained on an English vocabulary\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    \n",
    "    # Truncate sequences to be no longer than the maximum length set by the max_length parameter\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fae956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of examples using DataCollatorForSeq2Seq\n",
    "# dynamically pad the sentences to the longest length in a batch during collation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d590a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the SacreBLEU metric\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ba510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that passes model predictions and labels to compute() to calculate the SacreBLEU score\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# This function is called from training loop\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53347d67",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa11a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters in Seq2SeqTrainingArguments\n",
    "# The only required parameter is output_dir which specifies where to save model\n",
    "# NB: set push_to_hub=True (and sign in to Hugging Face) to upload model\n",
    "# At the end of each epoch, the Trainer will evaluate the SacreBLEU metric and save the training checkpoint\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"opus_translation_model\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Pass the training arguments to Seq2SeqTrainer...\n",
    "# along with the model, dataset, tokenizer, data collator, and compute_metrics function\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Call train on the trainer object to fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a6987",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to translate to another language\n",
    "# For T5, you need to prefix your input depending on the task you’re working on\n",
    "# For translation from English to French, you should prefix your input as shown below\n",
    "text = \"translate English to French: Coverage-directed test selection is the superior solution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> inference within a pipeline()\n",
    "translator = pipeline(\"translation\", model=\"opus_translation_model\")\n",
    "translator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035702e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> inference using PyTorch objects\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"opus_translation_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"opus_translation_model\")\n",
    "\n",
    "# Use the generate() method to create the translation\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "# Decode the generated token ids back into text\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
