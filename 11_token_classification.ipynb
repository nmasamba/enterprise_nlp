{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eac9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: evaluate in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: seqeval in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06104520",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cb6d3",
   "metadata": {},
   "source": [
    "Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Finetune DistilBERT on the WNUT 17 dataset to detect new entities.\n",
    "2. Use the finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94973ec9",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8ad32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90d10d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3729585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WNUT 17 dataset from the ðŸ¤— Datasets library\n",
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304e1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out an example\n",
    "wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d098035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('@paulwalk', 0), ('It', 0), (\"'s\", 0), ('the', 0), ('view', 0), ('from', 0), ('where', 0), ('I', 0), (\"'m\", 0), ('living', 0), ('for', 0), ('two', 0), ('weeks', 0), ('.', 0), ('Empire', 7), ('State', 8), ('Building', 8), ('=', 0), ('ESB', 7), ('.', 0), ('Pretty', 0), ('bad', 0), ('storm', 0), ('here', 0), ('last', 0), ('evening', 0), ('.', 0))\n"
     ]
    }
   ],
   "source": [
    "tokens_list = wnut[\"train\"][0]['tokens']\n",
    "ner_tags_list = wnut[\"train\"][0]['ner_tags']\n",
    "print(tuple(zip(tokens_list, ner_tags_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff2d188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each number in ner_tags represents an entity\n",
    "# Convert the numbers to their label names to find out what the entities are\n",
    "# NB: The letter that prefixes each ner_tag indicates the token position of the entity:\n",
    "# B - indicates the beginning of an entity.\n",
    "# I - token is contained inside the same entity (e.g. State token is a part of an entity like Empire State Building).\n",
    "# 0 indicates the token doesnâ€™t correspond to any entity\n",
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba304d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15a54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT tokenizer to preprocess the tokens field\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4bffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'paul',\n",
       " '##walk',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'empire',\n",
       " 'state',\n",
       " 'building',\n",
       " '=',\n",
       " 'es',\n",
       " '##b',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the example tokens field above, it looks like the input has already been tokenized...\n",
    "# But the input actually hasnâ€™t been tokenized yet\n",
    "# Weâ€™ll need to set is_split_into_words=True to tokenize the words into subwords\n",
    "example = wnut[\"train\"][0]\n",
    "example_tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "example_tokens = tokenizer.convert_ids_to_tokens(example_tokenized_input[\"input_ids\"])\n",
    "example_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the tokenisation process above introduces special characters such as [SEP] and [CLS]\n",
    "# Issue: this creates a mismatch between the input and labels. \n",
    "# Need to realign the tokens and labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Map all tokens to their corresponding word with the word_ids method\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Assign the label -100 to the special tokens [CLS] and [SEP]\n",
    "    # This causes them to be ignored by the PyTorch loss function (CrossEntropyLoss).\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd87636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessing function over the entire dataset\n",
    "# speed up the map function by setting batched=True to process multiple elements of the dataset at once\n",
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch of examples using DataCollatorWithPadding\n",
    "# Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation\n",
    "# Avoid pre-padding the whole dataset to the maximum length\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
