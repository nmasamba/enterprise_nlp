{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2c9a7d",
   "metadata": {},
   "source": [
    "Question answering tasks return an answer given a question, e.g. in asking a virtual assistant like Alexa, Siri or Google what the weather is, youâ€™ve used a question answering model before. \n",
    "\n",
    "There are two common types of question answering tasks:\n",
    "1. Extractive: extract the answer from the given context.\n",
    "2. Abstractive: generate an answer from the context that correctly answers the question.\n",
    "\n",
    "In this Huggingface-based notebook we:\n",
    "A) Finetune DistilBERT on the SQuAD dataset for **extractive** question answering.\n",
    "B) Use the finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9d5e7",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19f5a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: evaluate in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: backcall in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6e2daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 21:30:55.072901: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DefaultDataCollator, AutoModelForQuestionAnswering, \\\n",
    "TrainingArguments, Trainer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d8ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Huggingface to share model with the community\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f775f9",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d055bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a subset of the SQuAD dataset from the ðŸ¤— Datasets library for experimentation\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21d1cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57341835d058e614000b693f',\n",
       " 'title': 'Montana',\n",
       " 'context': \"The Yellowstone River rises on the continental divide near Younts Peak in Wyoming's Teton Wilderness. It flows north through Yellowstone National Park, enters Montana near Gardiner, and passes through the Paradise Valley to Livingston. It then flows northeasterly across the state through Billings, Miles City, Glendive, and Sidney. The Yellowstone joins the Missouri in North Dakota just east of Fort Union. It is the longest undammed, free-flowing river in the contiguous United States, and drains about a quarter of Montana (36,000 square miles (93,000 km2)).\",\n",
       " 'question': 'Where does the Yellowstone meet the Missouri river?',\n",
       " 'answers': {'text': ['North Dakota'], 'answer_start': [371]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "\n",
    "squad['train'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1900213",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e13f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a DistilBERT tokenizer to process the question and context fields\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc4ea6",
   "metadata": {},
   "source": [
    "There are a few preprocessing steps particular to question answering tasks you should be aware of:\n",
    "\n",
    "1. Some examples in a dataset may have a very long context that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the context by setting truncation=\"only_second\".\n",
    "2. Next, map the start and end positions of the answer to the original context by setting return_offset_mapping=True.\n",
    "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the sequence_ids method to find which part of the offset corresponds to the question and which corresponds to the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0a13f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0f8c96531a40f5b5fd09be1e7f1525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd5fccdd16e4a2d90795da9a6a2344f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function to truncate and map the start and end tokens of the answer to the context\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# apply the preprocessing function over the entire dataset using ðŸ¤— Datasets map function. \n",
    "# speed up the map function by setting batched=True to process multiple elements of the dataset at once \n",
    "# remove any columns you donâ€™t need\n",
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f76677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch of examples using DefaultDataCollator\n",
    "# Unlike other data collators in ðŸ¤— Transformers, the DefaultDataCollator does not apply any additional preprocessing such as padding\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ea40f",
   "metadata": {},
   "source": [
    "#Â Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a24bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT with AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d5f4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 07:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.119150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.655600</td>\n",
       "      <td>1.611291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.655600</td>\n",
       "      <td>1.515877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=2.228500691731771, metrics={'train_runtime': 446.0319, 'train_samples_per_second': 26.904, 'train_steps_per_second': 1.681, 'total_flos': 1175877900288000.0, 'train_loss': 2.228500691731771, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finetune model with Trainer\n",
    "# Use push_to_hub=True in TrainingArguments() if logged in to enable pushing to HF then\n",
    "# trainer.push_to_hub() after model is trained\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"question_answering_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23301761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# share your model to the Hub with the push_to_hub() method so everyone can use your model\n",
    "#trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3248620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below line can be used to manually generate a config.json file\n",
    "# model.config.to_json_file(\"question_answering_model/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae549b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation placeholder (expensive)\n",
    "# Visibility into model performance provided by trainer evaluation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59219afd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82aa6326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.14135223627090454,\n",
       " 'start': 10,\n",
       " 'end': 95,\n",
       " 'answer': '176 billion parameters and can generate text in 46 languages natural languages and 13'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a pipeline for question answering\n",
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=\"question_answering_model/checkpoint-500\")\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee157f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'176 billion parameters and can generate text in 13 programming languages and 46'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducing the pipeline\n",
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 13 programming languages and 46 natural languages.\"\n",
    "\n",
    "# tokenize the text and return PyTorch tensors:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"question_answering_model/checkpoint-500\")\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# pass inputs to the model and return the logits\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"question_answering_model/checkpoint-500\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# get the highest probability from the model output for the start and end positions\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "# decode the predicted tokens to get the answer\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5853c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
